{"cells":[{"cell_type":"markdown","id":"7684318f","metadata":{"id":"7684318f"},"source":["# 1. Setting up Environment"]},{"cell_type":"markdown","id":"24209ea4","metadata":{"id":"24209ea4"},"source":["## 1.1 Installing required libraries"]},{"cell_type":"code","execution_count":null,"id":"d80a0bf1","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d80a0bf1","outputId":"54661590-fbea-4b74-b397-2ca9aeae9fc2","tags":[]},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting nltk\n","  Downloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex>=2021.8.3 in ./miniconda3/envs/jupyter/lib/python3.10/site-packages (from nltk) (2023.8.8)\n","Requirement already satisfied: joblib in ./miniconda3/envs/jupyter/lib/python3.10/site-packages (from nltk) (1.2.0)\n","Requirement already satisfied: tqdm in ./miniconda3/envs/jupyter/lib/python3.10/site-packages (from nltk) (4.66.1)\n","Requirement already satisfied: click in ./miniconda3/envs/jupyter/lib/python3.10/site-packages (from nltk) (8.1.3)\n","Installing collected packages: nltk\n","Successfully installed nltk-3.8.1\n"]}],"source":["!pip install -q git+https://github.com/huggingface/transformers\n","!pip install -q datasets\n","!pip install -q -U  bitsandbytes==0.39.1 accelerate\n","!pip install -q einops\n","!pip install -q ijson\n","!pip install nltk\n","# !CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python"]},{"cell_type":"markdown","id":"6fb0dc5c","metadata":{"id":"6fb0dc5c"},"source":["## 1.2 Importing modules"]},{"cell_type":"code","execution_count":null,"id":"2bf7842f","metadata":{"id":"2bf7842f","tags":[]},"outputs":[],"source":["# Standard library imports for system and file operations\n","import gc\n","import os\n","from getpass import getpass\n","from time import time\n","import subprocess\n","import multiprocessing\n","import threading\n","\n","# Standard library imports for data manipulation and computation\n","import json\n","import random\n","import re\n","import string\n","from random import shuffle\n","\n","# Imports for logging and progress tracking\n","import logging\n","from IPython.display import clear_output\n","from tqdm import tqdm\n","\n","# Imports for natural language processing and machine learning\n","import ijson\n","import nltk\n","import torch\n","import transformers\n","from accelerate import Accelerator\n","from datasets import Dataset, load_dataset\n","from nltk.corpus import stopwords\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from huggingface_hub import hf_hub_download"]},{"cell_type":"markdown","id":"2556c8eb","metadata":{"id":"2556c8eb"},"source":["## 1.3 Setting Environment Variables"]},{"cell_type":"markdown","id":"6aee33dc","metadata":{"id":"6aee33dc"},"source":["# 2. Setting Up Configurations"]},{"cell_type":"code","execution_count":null,"id":"6450daa9","metadata":{"id":"6450daa9","tags":[]},"outputs":[],"source":["logger = logging.getLogger(\"evolInstructLogger\")\n","\n","# Create handlers\n","c_handler = logging.StreamHandler()\n","f_handler = logging.FileHandler('evolInstructLogger.log')\n","c_handler.setLevel(logging.DEBUG)\n","f_handler.setLevel(logging.DEBUG)\n","\n","c_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","f_format = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n","c_handler.setFormatter(c_format)\n","f_handler.setFormatter(f_format)\n","\n","# Add handlers to the logger\n","logger.addHandler(c_handler)\n","logger.addHandler(f_handler)"]},{"cell_type":"markdown","id":"23a2d71b","metadata":{"id":"23a2d71b"},"source":["## 2.1 Dataset Configurations"]},{"cell_type":"code","execution_count":null,"id":"c1d57965","metadata":{"id":"c1d57965","tags":[]},"outputs":[],"source":["## Datasets\n","datasets = [\n","    \"databricks/databricks-dolly-15k\"\n","]"]},{"cell_type":"markdown","id":"0ef2653f","metadata":{"id":"0ef2653f"},"source":["## 2.2 Model Configurations"]},{"cell_type":"code","execution_count":null,"id":"bde922fb","metadata":{"id":"bde922fb","tags":[]},"outputs":[],"source":["# Model used for generating the dataset\n","falcon_model_GGML = \"TheBloke/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML\"\n","falcon_model_file_GGML = \"h2ogpt-falcon-40b.ggmlv3.q6_k.bin\"\n","\n","falcon_model_path = hf_hub_download(\n","    falcon_model_GGML,\n","    falcon_model_file_GGML\n",")\n","\n","# Model used for evaluating evolved instructions\n","llama_model_GGML = \"TheBloke/Llama-2-70B-chat-GGML\"\n","llama_model_file_GGML = \"llama-2-70b-chat.ggmlv3.q4_0.bin\"\n","\n","llama_model_file_GGUF = \"llama-2-70b-chat.ggufv3.q4_1.bin\"\n","\n","llama_model_path_GGML = hf_hub_download(\n","    llama_model_GGML,\n","    llama_model_file_GGML\n",")\n","\n","llama_model_path_GGUF = os.path.join(os.path.dirname(llama_model_path_GGML), llama_model_file_GGUF)"]},{"cell_type":"markdown","id":"0dee8fb9","metadata":{"id":"0dee8fb9"},"source":["# 3. Preparation"]},{"cell_type":"markdown","id":"ca97406c","metadata":{"id":"ca97406c","tags":[]},"source":["## Falcon Model"]},{"cell_type":"code","execution_count":null,"id":"5709a006","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["5fc100dcdd834ac59ea4df36e77a50d6","9696756182434f978ce452b224b8107f","dca9feae783f4691a05996a805b93113","4b422704f20a4c7ba663750ce57bce66","16b20b7daa5c4b67ba8da5857c40ecb3","1fbe79698c6d4b0eb5f0bfd87e536e3d","a6f579122ad844b88b4a41a158290430","ef2d3b165b8947e580e9f1d3b9673116","da5241f3e6cd4f8882bf4dbbfd5f7ac0","ffa9d39a538448c2a0fddf39fe7217c0","3bfa0673f7ce46a49b4e5d996f0c9fc8","1ad3f195c6d74e878f213c4cbd9ef6fe","e78842f33b3049ea91272d81be7fc4e7","06faf1c546454096a8a4beb98e03c5c5","2f6cea45b119444b8815e06384f007fa","68cc66268ecf4bde994f1e67bab77f63","a7a317bc7d0c42b5b48eec8324c117d8","5863f42be4384e56b6e65e05b2fed7ee","f7e7041afba343acaf88a274a2244a32","239d29edb95645698eab074487a680d7","300182b9a9a0467db44edb46b5d40940","f16cd0a15c2c46a9beaf355865de2cc5","82a490d7f4bd4ce090026439694e1529","f7557e29e62b484fbd6030ed22852c22","9976f1e2368b4d44a32a4212dab0d6ee","3c4b60a636114357b10af782e9560b91","a0cdf064253946b1bc1010b985f0b307","6ff30ba1517247f3b9e045b0f011cc4a","b7f92eb7e47d4369b362c82b74fb08b1","520f9ace6d3b4dd1aeb8aa8ebe759f6a","413207ad55ff49718a2791928b0fdbbb","bfbe8768a3214a3c8589fec1116a1fa3","bc1f5bd885dd461494d6fe1f5373db16","af8f68aa2a6f45d6a01f00015f38f92c","6387ccfb370448848902e6019c112068","75e59ff08688487f99d62d1f83a2bc59","84ad8b3664d7475785f88ca5bf056b65","6828a3886a524f86b8aba34ea1019b3f","23f2780fbdc04b7eaeaff1f14a9ef8eb","e171660b7a30484ebe9f8ab4c901c844","d58f91667061405282fd2bf302935550","25ae05aa696d43a98afbdb701b38c26d","3c9052cba4aa4c16bc1aaa8daeb21d35","272b4c65d326400d824f27c23b5780bf","4791acc47cc04e15b8e54c3bcd43072c","85f2ccd4a87f4d619860acb1709b89d2","7d7afbc2a5e4426486ac74e50e557152","8b9470faecd34268b0ada76d166bbaef","00d9266f40f145e0b1c1795716cc9a55","d2bb374075e54906b08a7b0ad4fe20af","858447b51d2741099c3008c1459dd828","efec8ffac0d748da981cbf49f46946fe","4de93a5d445448d3b066a39d15ea628f","c21481ab20a041089fbd2adf2c516227","f992bb88651f4b39b45d9acf52a3d39d","5616d87393ca4eb291b14503e61de714","109c18d851f0446791951b861bb9feb8","91920cab969f4dbea7a410840306d46c","f3a913fc93db4c3cbac4995c18fec5d4","892beb848a0d4dff8760fc218115c41e","80f61d3249074f52b7fd7fbe90287056","1caa7f14ec5c4b99bc23ed143f819a66","96f45413b69a48eda7220db015130d71","86149acb98df4df4b4f8ea7ed47280f2","08224094f3604aa49b12e24495895cb0","97eba67cfd8843788308a2874a50c7a8","704ab25ba9f542088936ba1486d1d1a3","04b0cdf6f0fb44b58ec0d63e60c50321","70135362f9ff488ba29825670da3c49d","9910695a80e74c2c8fed08168377b3ed","9d518e8054fe4021896595d4659e10e5","9678052c2cc945a4862e20b83ddc8403","d984e60caad34728b68c82c428d8ed4f","91fe89edc45c400aa7974db5a1b6019f","91d8f8e44cbf488386de683c8f294dff","8ba651cca3e6487a8c65717d0d70b69b","7efb10f15ae34dc6a8c0b7ef0332154a","e53df9b31ba44c9299639f19b0032a92","23bf90757095425b85c16d6b58fe349c","7e31be01096c4898919f8db3fa287433","35c6b021a337482998c5970c87323f0f","41530b1cfddd4920983065c725075f37","5a4f70fe88e149caa501b9ed1f401de6","fbf8a5d2c73b45d29a9669b88235fc44","806959671fc046188baf13e8a9883d1f","1dd2ca4567d549db832b65bdec0f30ba","049d93b1155543c78cec6a9fd34d1029","6f29a8b0f70240aa86f855e11da96293","449b46ea997241fdb041b6a2616df1cd","be43e3247000473ab5a3ef3340bf3fa9","93ad4f8e80134bd99bc69cff63715020","d6ea9416403946e5920109f2d52c05f3","ee5333d4bb7c49259c8fe39a5a8fe362","3a83d1cce8c449a2a9c9cb2fe5e97d5f","ecd830acf3eb4ba1a27a69d37a2a2f66","422fd97d161d42e7a84cae82c536e79d","9a94708c76404d89b44be4ef60007ee3","b7892795e3934ceabe14ab8b51618bcc","0bf6beae34b344438cb351cec034df06","c71019aa89be404cb86c611f16904aa9","35804ccd1a2d4eb195cdcfd2cf0b9238","126854b1066242e58019ece6332db7fc","6d4c517aa72143c7b62375dc7701cfb4","54e7a5dc3acc4f10a28b5e4ea5b2ddda","0918b1d44ef241e29c1d1b1e963cb311","55aa1dd5ede142519a58dfcee617d5b8","df5b3b1ab07a42f28ddba7398123a7bc","8eabd7a5f2474ac6a8230627622f810a","468d3d5f0ea140c9927e27a43b55c7ef","e4a0ff03679b45178bd036ba57392ded","20347d15e08f4cc7a68480cb5026d61e","c9b4520fd6594c519cbd402ef0371ca0","05bbf0fbddcf4d8890b7c621492fbc14","ebe512c4a0cf47e998371d48d571ab5a","df4162a726254be1a957e450db00f84a","8446a7f8235c4a838833525d33c0e52b","e9249fb2190b422d8671260bcf65843b","215e42249164449ca273b90a5c0da7a0","839a5ac7abb04f7d9a31a976dce0425d","7a20491e0d6d476caa374bd4d1969384","434b5ebb8fee45b2a2dd9cca07d65e15","85eefce63f6845a5ad20bd7ddbea75fa","6f8c88afbce74813b85515f4de2caf7f","17a218f3495c4222a5c617781091f9b1","928e66ff5d054a2b94a34e372e1e28df","260146531e7647b5bdc7758bd8036a03","f37b53fe93f642f7bd28abea9c856f22","4c393510674640f79f2df19a799e1e32","0cf54f0281c244c4b1f122f8ba8df673","2cd5b2eef5bd4e39a60ff5cb1cc6e10e","684183c7caf44f9abf5321378c722262","6487ba202a8f4c93b836089c18fab808","b35904e1939a487c8d0e54987e66d31c","0361056785954945836d15ab3b4e8b8f","3f9a38d3bef54765abb43d2e621b52ae","296069b311f7498caa60d803770fca27","953c245277b948e292c6a78c056516e2","39ad50abc0f7413fb09428fe013933e4","5df06a1504b0415e98cce6cb4e0824e0","8cef6922f9ed464abf70dbd51cc72e79","1c3d502ced0c47d990920ccf9c18aa0c","8f696265f73a4159b3031e460fcf5ef8","25d7aba4525c4a28bd6448dfe220789d","d7f088fb4f5a492db7e58e72adaeecfc","7d243146903b404a9506ad1c557b256c","a8cc91d841bd42839cd4766ec147f779","c42b20bf8b1a472b8b3623c62210e03e","75de6af46d57470e92fa5a98e6e30cc6","c4d0090e087a43c08e1612df73ae530c","3616736e537147c38ec233e01d859cc6","c8fb1d57a92a438ea3154020e50279fe","5e0275846b844e7481348cb966aee269","372cded900504ccbb06b1abd5a82045f","36e1e847b19940b782a68c570edecd9f","754f6859b4004211bf4b5f05fb90b454","5d8a9098c4dd412a8aa3560bbe302d31","15ed9bc814a645a99412eca9c1623923","7a9ebc759f5646b4b08183a2a598dcc2","8eecf985018d4d48ba0b46f290597208","02c116394b354d08b49152ed4c6632b0","e15de74bf85a495da5df0e1d07939720","7f1704ba8eb945bc9a35007e73839d97","5df6ccbe4c824ef4ac8907ea29a48de9","dfda2bbc7ac143b3b0f7bff416c7e0c6","3b6b7fa679cf46e7965bc14fbdffe5aa","ed63101bb240487c9af22900052f5587","b01fe27c34e74f77af601baf4fe5a5a4","2a8c8312e4444e8c9c26434bcfe7336e","bb0e309bc3aa4a52bbdc4c3473047e07","5c1b4545b5d549298cf54be82e0da171","7a13d81fad27479ab83f90045faf91d0","28b6366aa2af4a8599c4471f08614d80","f0bf4bcbd7d54288a351fab18b12e21d","2a7e78da48ee461b9bcd098e614cc134","c784624275e14e94acb89c1f55e352e4","3d2d5bf92bbd486995ea96f5ad00f298","28eaf4f91b7f40ef9d5bd08e23e799bf","bfc8b7969cfd4f358c56bb87323ed9ce","5ed9d32da5134aad84bb79ae252d1365","9daaaa55f3a542d48a9967c0b9c2c7d0","61258e0e041f4cc49f3599099ccd4ee8","d1319b47c9734e618f7a62e4cce2c1d2","93f62d05a2374a8b9fb1b33dc627c8ce","9f486da488b640c2a56d0bde45d86e4e","c12e400782d74a398c60c929db26469c","f098df421b59445cb1f55780db7dc48b","a6233308434a4ec292dfb6352903eec4","f2856496448b4645a1427e206fd635cb","18d85e24c5a7485bb2e43169933e048d","595910686ea846f2b7f9a32b236dad03","47334989b4564e929a818b7dc534f5fd","8a48a6701eab45ada29a5629ebc48631","8b73930c2fe34f7c90ff297f3e9a89bd","c603382926714eefb4b7ee0b1104737d","8c527f62ebf345ec8092cdd36ccb29be","8a9d3f6cefd2444ea92be1de5c97dd67","9d67419e05984c20a217dda2307cc86c","40836cd127d541aeaca3fbe022bde030","cf8b3acedd0047b196d8a58be42c8a31","abe008dad489412eb6c3ca609dc42cf3","051ba89223b5406299b642dad7220c14","89d56ead40c04e309daa3783e4b4c3df"]},"id":"5709a006","outputId":"f0d7bc36-c6bc-4549-ca82-b81e621cd866"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'ggllm.cpp'...\n","remote: Enumerating objects: 4273, done.\u001b[K\n","remote: Counting objects: 100% (1561/1561), done.\u001b[K\n","remote: Compressing objects: 100% (99/99), done.\u001b[K\n","remote: Total 4273 (delta 1498), reused 1470 (delta 1462), pack-reused 2712\u001b[K\n","Receiving objects: 100% (4273/4273), 107.45 MiB | 15.41 MiB/s, done.\n","Resolving deltas: 100% (2889/2889), done.\n","/home/user/.local/share/Trash/files/evol-instruct/ggllm.cpp\n","I ggllm.cpp build info: \n","I UNAME_S:  Linux\n","I UNAME_P:  x86_64\n","I UNAME_M:  x86_64\n","I CFLAGS:   -I.              -O3 -std=c11   -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n","I CXXFLAGS: -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include\n","I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n","I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n","I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n","\n","cc  -I.              -O3 -std=c11   -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c ggml.c -o ggml.o\n","\u001b[01m\u001b[Kggml.c:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_compute_forward_mul_mat_f32\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kggml.c:10924:19:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kne10\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n","10924 |     const int64_t \u001b[01;35m\u001b[Kne10\u001b[m\u001b[K = src1->ne[0];\n","      |                   \u001b[01;35m\u001b[K^~~~\u001b[m\u001b[K\n","g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c libfalcon.cpp -o libfalcon.o\n","\u001b[01m\u001b[Klibfalcon.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kbool falcon_eval_internal(falcon_context&, const falcon_token*, falcon_evaluation_config&)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Klibfalcon.cpp:2138:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Koffload_func_nr\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n"," 2138 |     offload_func_t \u001b[01;35m\u001b[Koffload_func_nr\u001b[m\u001b[K = llama_nop; // nr = non-repeating\n","      |                    \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Klibfalcon.cpp:2139:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Koffload_func_kqv\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n"," 2139 |     offload_func_t \u001b[01;35m\u001b[Koffload_func_kqv\u001b[m\u001b[K = llama_nop;\n","      |                    \u001b[01;35m\u001b[K^~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Klibfalcon.cpp:2412:20:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Koffload_func\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n"," 2412 |     offload_func_t \u001b[01;35m\u001b[Koffload_func\u001b[m\u001b[K = llama_nop;\n","      |                    \u001b[01;35m\u001b[K^~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Klibfalcon.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ksize_t falcon_copy_state_data(falcon_context*, uint8_t*)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Klibfalcon.cpp:4276:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn_embd\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n"," 4276 |         const int    \u001b[01;35m\u001b[Kn_embd\u001b[m\u001b[K  = hparams.n_embd;\n","      |                      \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Klibfalcon.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Ksize_t falcon_set_state_data(falcon_context*, uint8_t*)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Klibfalcon.cpp:4404:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn_embd\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n"," 4404 |         const int    \u001b[01;35m\u001b[Kn_embd\u001b[m\u001b[K  = hparams.n_embd;\n","      |                      \u001b[01;35m\u001b[K^~~~~~\u001b[m\u001b[K\n","g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c cmpnct_unicode.cpp -o cmpnct_unicode.o\n","g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -c examples/falcon_common.cpp -o falcon_common.o\n","In file included from \u001b[01m\u001b[Kexamples/falcon_common.h:6\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[Kexamples/falcon_common.cpp:1\u001b[m\u001b[K:\n","\u001b[01m\u001b[K./libfalcon.h:272:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KFINETUNE_NAME\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n","  272 |     static const char *\u001b[01;35m\u001b[KFINETUNE_NAME\u001b[m\u001b[K[8] = { \"UNSPECIFIED\", \"NONE\", \"ALPACA\", \"OPENASSISTANT\", \"OPENASSIST_V1\", \"WIZARD\", \"FALCONINSTRUCT\", \"OPENBUDDY\"};\n","      |                        \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n","cc -I.              -O3 -std=c11   -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include   -c -o k_quants.o k_quants.c\n","nvcc --forward-unknown-to-host-compiler -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_DMMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n","g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/falcon/falcon_main.cpp ggml.o libfalcon.o cmpnct_unicode.o falcon_common.o k_quants.o ggml-cuda.o -o falcon_main  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n","\u001b[01m\u001b[Kexamples/falcon/falcon_main.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kint main(int, char**)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kexamples/falcon/falcon_main.cpp:714:58:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Ksize_t\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} and ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","  714 |                     if (params.n_keep > 0 && \u001b[01;35m\u001b[Kprompt_size > n_system\u001b[m\u001b[K)\n","      |                                              \u001b[01;35m\u001b[K~~~~~~~~~~~~^~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kexamples/falcon/falcon_main.cpp:729:33:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n","  729 |                     if (\u001b[01;35m\u001b[Kn_regen > all_generation.size()-embd.size()\u001b[m\u001b[K) n_regen = (int)all_generation.size()-(int)embd.size();\n","      |                         \u001b[01;35m\u001b[K~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kexamples/falcon/falcon_main.cpp:1202:79:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kcomparison of integer expressions of different signedness: ‘\u001b[01m\u001b[Kint\u001b[m\u001b[K’ and ‘\u001b[01m\u001b[Kstd::vector<int>::size_type\u001b[m\u001b[K’ {aka ‘\u001b[01m\u001b[Klong unsigned int\u001b[m\u001b[K’} [\u001b[01;35m\u001b[K-Wsign-compare\u001b[m\u001b[K]\n"," 1202 | d.empty() && embd.back() == falcon_token_eos() && \u001b[01;35m\u001b[Kn_consumed >= embd_inp.size()\u001b[m\u001b[K) || stopword_fulfilled)\n","      |                                                   \u001b[01;35m\u001b[K~~~~~~~~~~~^~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\n","\u001b[01m\u001b[Kexamples/falcon/falcon_main.cpp:149:22:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kvariable ‘\u001b[01m\u001b[Kctx_system\u001b[m\u001b[K’ set but not used [\u001b[01;35m\u001b[K-Wunused-but-set-variable\u001b[m\u001b[K]\n","  149 |     falcon_context * \u001b[01;35m\u001b[Kctx_system\u001b[m\u001b[K = nullptr;\n","      |                      \u001b[01;35m\u001b[K^~~~~~~~~~\u001b[m\u001b[K\n","\u001b[01m\u001b[Kexamples/falcon/falcon_main.cpp:648:9:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused variable ‘\u001b[01m\u001b[Kn_past_system\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n","  648 |     int \u001b[01;35m\u001b[Kn_past_system\u001b[m\u001b[K      = 0; // not in use\n","      |         \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n","\n","====  Run ./falcon_main -h for help.  ====\n","Read the readme file for important parameters and usage\n","\n","g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/falcon_quantize/quantize.cpp ggml.o libfalcon.o cmpnct_unicode.o k_quants.o ggml-cuda.o -o falcon_quantize  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n","In file included from \u001b[01m\u001b[Kexamples/falcon_quantize/quantize.cpp:3\u001b[m\u001b[K:\n","\u001b[01m\u001b[K./libfalcon.h:272:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KFINETUNE_NAME\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n","  272 |     static const char *\u001b[01;35m\u001b[KFINETUNE_NAME\u001b[m\u001b[K[8] = { \"UNSPECIFIED\", \"NONE\", \"ALPACA\", \"OPENASSISTANT\", \"OPENASSIST_V1\", \"WIZARD\", \"FALCONINSTRUCT\", \"OPENBUDDY\"};\n","      |                        \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n","g++ -I. -I./examples -O3 -std=c++11 -fPIC -DNDEBUG -DGGML_PERF=1 -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -pthread -march=native -mtune=native -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include examples/falcon_perplexity/falcon_perplexity.cpp ggml.o libfalcon.o falcon_common.o cmpnct_unicode.o k_quants.o ggml-cuda.o -o falcon_perplexity  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib\n","In file included from \u001b[01m\u001b[K./examples/falcon_common.h:6\u001b[m\u001b[K,\n","                 from \u001b[01m\u001b[Kexamples/falcon_perplexity/falcon_perplexity.cpp:1\u001b[m\u001b[K:\n","\u001b[01m\u001b[K./libfalcon.h:272:24:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[KFINETUNE_NAME\u001b[m\u001b[K’ defined but not used [\u001b[01;35m\u001b[K-Wunused-variable\u001b[m\u001b[K]\n","  272 |     static const char *\u001b[01;35m\u001b[KFINETUNE_NAME\u001b[m\u001b[K[8] = { \"UNSPECIFIED\", \"NONE\", \"ALPACA\", \"OPENASSISTANT\", \"OPENASSIST_V1\", \"WIZARD\", \"FALCONINSTRUCT\", \"OPENBUDDY\"};\n","      |                        \u001b[01;35m\u001b[K^~~~~~~~~~~~~\u001b[m\u001b[K\n","/home/user/.local/share/Trash/files/evol-instruct\n"]},{"name":"stderr","output_type":"stream","text":["--2023-09-08 18:13:28--  https://huggingface.co/h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2/resolve/main/tokenizer.json\n","Resolving huggingface.co (huggingface.co)... 18.165.122.30, 18.165.122.120, 18.165.122.101, ...\n","Connecting to huggingface.co (huggingface.co)|18.165.122.30|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 2734158 (2.6M) [text/plain]\n","Saving to: ‘/home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/tokenizer.json’\n","\n","     0K .......... .......... .......... .......... ..........  1% 37.5M 0s\n","    50K .......... .......... .......... .......... ..........  3% 38.9M 0s\n","   100K .......... .......... .......... .......... ..........  5%  154M 0s\n","   150K .......... .......... .......... .......... ..........  7%  123M 0s\n","   200K .......... .......... .......... .......... ..........  9% 63.4M 0s\n","   250K .......... .......... .......... .......... .......... 11% 99.7M 0s\n","   300K .......... .......... .......... .......... .......... 13%  103M 0s\n","   350K .......... .......... .......... .......... .......... 14% 59.3M 0s\n","   400K .......... .......... .......... .......... .......... 16% 38.1M 0s\n","   450K .......... .......... .......... .......... .......... 18%  136M 0s\n","   500K .......... .......... .......... .......... .......... 20% 51.8M 0s\n","   550K .......... .......... .......... .......... .......... 22% 70.7M 0s\n","   600K .......... .......... .......... .......... .......... 24% 45.6M 0s\n","   650K .......... .......... .......... .......... .......... 26% 77.5M 0s\n","   700K .......... .......... .......... .......... .......... 28% 50.1M 0s\n","   750K .......... .......... .......... .......... .......... 29% 63.6M 0s\n","   800K .......... .......... .......... .......... .......... 31% 46.6M 0s\n","   850K .......... .......... .......... .......... .......... 33% 38.9M 0s\n","   900K .......... .......... .......... .......... .......... 35%  124M 0s\n","   950K .......... .......... .......... .......... .......... 37% 47.3M 0s\n","  1000K .......... .......... .......... .......... .......... 39% 75.5M 0s\n","  1050K .......... .......... .......... .......... .......... 41% 34.6M 0s\n","  1100K .......... .......... .......... .......... .......... 43% 92.9M 0s\n","  1150K .......... .......... .......... .......... .......... 44% 52.4M 0s\n","  1200K .......... .......... .......... .......... .......... 46% 38.6M 0s\n","  1250K .......... .......... .......... .......... .......... 48% 99.1M 0s\n","  1300K .......... .......... .......... .......... .......... 50% 21.4M 0s\n","  1350K .......... .......... .......... .......... .......... 52%  107M 0s\n","  1400K .......... .......... .......... .......... .......... 54%  139M 0s\n","  1450K .......... .......... .......... .......... .......... 56%  133M 0s\n","  1500K .......... .......... .......... .......... .......... 58%  130M 0s\n","  1550K .......... .......... .......... .......... .......... 59% 88.9M 0s\n","  1600K .......... .......... .......... .......... .......... 61% 47.9M 0s\n","  1650K .......... .......... .......... .......... .......... 63% 52.4M 0s\n","  1700K .......... .......... .......... .......... .......... 65% 60.5M 0s\n","  1750K .......... .......... .......... .......... .......... 67% 56.9M 0s\n","  1800K .......... .......... .......... .......... .......... 69% 53.9M 0s\n","  1850K .......... .......... .......... .......... .......... 71% 78.4M 0s\n","  1900K .......... .......... .......... .......... .......... 73% 93.1M 0s\n","  1950K .......... .......... .......... .......... .......... 74% 48.5M 0s\n","  2000K .......... .......... .......... .......... .......... 76% 45.5M 0s\n","  2050K .......... .......... .......... .......... .......... 78% 85.0M 0s\n","  2100K .......... .......... .......... .......... .......... 80% 55.9M 0s\n","  2150K .......... .......... .......... .......... .......... 82% 91.3M 0s\n","  2200K .......... .......... .......... .......... .......... 84% 27.3M 0s\n","  2250K .......... .......... .......... .......... .......... 86%  101M 0s\n","  2300K .......... .......... .......... .......... .......... 88% 23.1M 0s\n","  2350K .......... .......... .......... .......... .......... 89% 71.8M 0s\n","  2400K .......... .......... .......... .......... .......... 91% 31.5M 0s\n","  2450K .......... .......... .......... .......... .......... 93% 42.5M 0s\n","  2500K .......... .......... .......... .......... .......... 95% 43.5M 0s\n","  2550K .......... .......... .......... .......... .......... 97% 35.8M 0s\n","  2600K .......... .......... .......... .......... .......... 99% 83.1M 0s\n","  2650K .......... ..........                                 100%  263M=0.05s\n","\n","2023-09-08 18:13:29 (56.0 MB/s) - ‘/home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/tokenizer.json’ saved [2734158/2734158]\n","\n"]},{"data":{"text/plain":["0"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["!git clone https://github.com/cmp-nct/ggllm.cpp\n","%cd ggllm.cpp\n","os.environ['LLAMA_CUBLAS'] = '1'\n","os.environ['PATH'] = \"/usr/local/cuda/bin:/usr/local/cuda/bin:/home/user/miniconda3/envs/jupyter/bin:/home/user/miniconda3/condabin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/snap/bin\"\n","!make falcon_main falcon_quantize falcon_perplexity\n","%cd ..\n","\n","\n"]},{"cell_type":"markdown","id":"ca2bf93b","metadata":{"tags":[],"id":"ca2bf93b"},"source":["## Llama 2 Model"]},{"cell_type":"code","execution_count":null,"id":"8288d51f","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":273,"referenced_widgets":["f58f5cfb80454f4cace1b38d5263cfe4","fd3ccdf97b0a46009c71dbdb0e6fac82","5ea1450ca4a84b66a0311a83f44f26be","acd6460a21a84b60b9dc03dd2344af10","87658712906c4cafbd26aad7e7fbe43a","12df5fc5ba1148a5862d08c2dd70b55c","c906317566df457382ec4ca6a2870d0f","ecbf59364015403a8b8d4d72f9725ef9","92fb6fe92d66430fbfba450ffcfd8b92","340791dbd4c64ec1b40a53451b488f96","6ecdc7aa6e804529a1710cd5f28d7894","f0556690872c4ac38d7fbc8179ebd1b1","584979f8446243ccbe18a3e3917e1284","1cd56490f6d548489dce9b911e1ac5ed","ab759b7aefa24c81b9c6e557bfb7b66e","3eba36f6031f4c8fa0a5bce83bef572d","41f53055778647a1aefeeb0ea8b7380e","3d42cedb7caa48b7b825bd46e52da9ab","ec56acf0d8784a0f9104607e8501c068","5a98addb95ce4d80b61c62045d9ceffa","a1a709027e7e4a739c8fa2228a06d7e4","0f53b6f15f0c48fdb4f9aa315f28b79c","332bc145c7ae4b019e788dabdc60557d","09079bff7ffb4ae6865a9107ace6c865","43504acf3b894d8cb72c7eefc653db55","70e253c1f766446c8f057273c189e5f0","9964cd4a54284ccda26591e73ea1610f","6fc33f5d474640b6b8bf49e581289f88","3b389b9e598e4f239649b2b98b2703ad","1ad7f7d48eb14b6b8c3039e5bf355f74","21af34db439c4282beee78e2bf8372b0","002ebf0d34254b0f915e9d4bf8160e5a","eebc48d8c998435798102d7257231506","48ac135e58a14210b1bb89b5ab9a0535","d26afd6b2f034e03b27a16c6a43000c9","55d10b4132f14cf5b7a9cdd4458b29a4","8890a5511e4d4a84a430c2386fad3cd5","5ae64e9b921b44fbab135c8de68f13f6","637ecc246510466489a8afec83ac9209","4cfff59f76d74fd78fb13d80b3b1cb4e","4362b159ce4c453593983de0b28676b2","169f48d8f7774e14ad8da2daf2465bda","0bdfe70ba2c248f0810e250e979d224c","358ddd1cb6724195ac4d7d0951e484d3","77517f6ef1e74c64a04d6fe1bad8eb18","bc000ef2dd3944c59491bc0981811071","1122dc09b3e94cd1882d1f7934ac3c02","66afcd3e0f1b458ebbb3a5d1fb2a4dd7","6fea0cc016484f63b74682f4fb18197e","d44ee5c540df44e289097e812e5f5499","0e4cbf7baa374ef297f3cc4751f214db","a964a9926a33448da7cab9266d786eab","4a6ac861ccb94358988434705cd830b9","703efd78352a415fbfa9aea91dc740f4","fd39d28448744f93a3d681ca10c5a5b3","5ce3421835f84c83a97fab694c791b1a","4bdb85b31c7e4515af143db42ff2e67a","73acf91232804d06864b9d30e8473788","608624e395264de494f0da588428064a","d74e1b1d48224d2c8eb21e53c6fe12b5","b3305faef72d4904bdc9fdd03ea191c1","08636293a8cb4a8990f63290f47fbaac","1d07d390f3e4487086a2c2d02b0e86f4","e4c86786f4c6484586b0cd6499653d05","3e1c9391147b42eebb856a107967f217","d22ec908af1348eca671f2a55c2629e5","b29901edebbf410dba35a850453c67ab","3a8296d53f96484eac8eb64c6b7f5a6d","f93a5bf7b3874e4ba2664938bfa20c68","8b1ea2d580db41b9a719162c222c6715","30a78c42a7824ba383b1c7123f760d72","152d2e35789144339ea8fff661cffb63","b122d287e0f441b59ee2947b2a17f0fd","291e1fdd79134623adc11cce95e66107","336ec052a3af40bd98d995bd614bd522","cfc41359898648d2afef42c33f34ec69","59204f14875b4ed9934532be58e6af97","7016497fecff4fb9b95a79b4c4d0e566","89dcb66db969421bb6a630ddba07c0cd","d882d39be9b44206b25855b4e21aaac2","c2247eb474724398b133fcd9ed27f9c8","718fc11c91c54b7ea18ccef8b53eb0ff","fef1da6478e74b91a483f16e59ef43e7","a73cc50fb375492b895b4c1c5a3796d0","5aabaf9d9fb342f1b3a843c80aaa1f94","17174d8998554a7dac7837f8f1eb8247","f2db9a89da294fd1849961a438f17388","e687c88b72544b93b67ea13de692c909"]},"id":"8288d51f","outputId":"cfa26873-070a-44d1-bbf1-c3e2f48322c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Cloning into 'llama.cpp'...\n","remote: Enumerating objects: 8602, done.\u001b[K\n","remote: Counting objects: 100% (4256/4256), done.\u001b[K\n","remote: Compressing objects: 100% (366/366), done.\u001b[K\n","remote: Total 8602 (delta 4072), reused 3935 (delta 3890), pack-reused 4346\u001b[K\n","Receiving objects: 100% (8602/8602), 7.80 MiB | 24.95 MiB/s, done.\n","Resolving deltas: 100% (5976/5976), done.\n","/home/user/llama.cpp\n","I llama.cpp build info: \n","I UNAME_S:  Linux\n","I UNAME_P:  x86_64\n","I UNAME_M:  x86_64\n","I CFLAGS:   -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -pthread -march=native -mtune=native \n","I CXXFLAGS: -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native \n","I LDFLAGS:   -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","I CC:       cc (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n","I CXX:      g++ (Ubuntu 9.4.0-1ubuntu1~20.04.1) 9.4.0\n","\n","cc  -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -pthread -march=native -mtune=native    -c ggml.c -o ggml.o\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  -c llama.cpp -o llama.o\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  -c common/common.cpp -o common.o\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  -c common/console.cpp -o console.o\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  -c common/grammar-parser.cpp -o grammar-parser.o\n","cc -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -pthread -march=native -mtune=native  -c k_quants.c -o k_quants.o\n","nvcc --forward-unknown-to-host-compiler -use_fast_math -arch=native -DGGML_CUDA_DMMV_X=32 -DGGML_CUDA_MMV_Y=1 -DK_QUANTS_PER_ITERATION=2 -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  -Wno-pedantic -c ggml-cuda.cu -o ggml-cuda.o\n","\u001b[01m\u001b[Kggml-cuda.cu:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kvoid ggml_cuda_op_alibi(const ggml_tensor*, const ggml_tensor*, ggml_tensor*, char*, float*, float*, float*, int64_t, int64_t, int64_t, int, CUstream_st*&)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kggml-cuda.cu:5754:58:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kunused parameter ‘\u001b[01m\u001b[Ki02\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wunused-parameter\u001b[m\u001b[K]\n"," 5754 |     float * src0_ddf_i, float * src1_ddf_i, float\u001b[01;35m\u001b[K * dst_ddf_\u001b[m\u001b[Ki, int64_t i02, int64_t i01_low, int64_t i01_high, int i1,\n","      |                                                  \u001b[01;35m\u001b[K~~~~~~~~^~~\u001b[m\u001b[K\n","cc  -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -pthread -march=native -mtune=native    -c ggml-alloc.c -o ggml-alloc.o\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/main/main.cpp ggml.o llama.o common.o console.o grammar-parser.o k_quants.o ggml-cuda.o ggml-alloc.o -o main  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","\n","====  Run ./main -h for help.  ====\n","\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/quantize/quantize.cpp ggml.o llama.o k_quants.o ggml-cuda.o ggml-alloc.o -o quantize  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/quantize-stats/quantize-stats.cpp ggml.o llama.o k_quants.o ggml-cuda.o ggml-alloc.o -o quantize-stats  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/perplexity/perplexity.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o perplexity  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/embedding/embedding.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o embedding  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  pocs/vdot/vdot.cpp ggml.o k_quants.o ggml-cuda.o ggml-alloc.o -o vdot  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/train-text-from-scratch/train-text-from-scratch.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o train-text-from-scratch  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:500:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n","  500 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n","      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:597:2:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[Kextra ‘\u001b[01m\u001b[K;\u001b[m\u001b[K’ [\u001b[01;35m\u001b[K-Wpedantic\u001b[m\u001b[K]\n","  597 | }\u001b[01;35m\u001b[K;\u001b[m\u001b[K\n","      |  \u001b[01;35m\u001b[K^\u001b[m\u001b[K\n","\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:\u001b[m\u001b[K In function ‘\u001b[01m\u001b[Kggml_tensor* llama_build_train_graphs(my_llama_model*, ggml_allocr*, ggml_context*, ggml_cgraph*, ggml_cgraph*, ggml_cgraph*, ggml_tensor**, ggml_tensor*, ggml_tensor*, int, int, bool, bool)\u001b[m\u001b[K’:\n","\u001b[01m\u001b[Kexamples/train-text-from-scratch/train-text-from-scratch.cpp:735:68:\u001b[m\u001b[K \u001b[01;35m\u001b[Kwarning: \u001b[m\u001b[K‘\u001b[01m\u001b[Kkv_scale\u001b[m\u001b[K’ may be used uninitialized in this function [\u001b[01;35m\u001b[K-Wmaybe-uninitialized\u001b[m\u001b[K]\n","  735 |       struct ggml_tensor * t16_1 = \u001b[01;35m\u001b[Kggml_scale_inplace        (ctx, t16_0, kv_scale)\u001b[m\u001b[K;          set_name(t16_1, \"t16_1\"); assert_shape_4d(t16_1, N, N, n_head, n_batch);\n","      |                                    \u001b[01;35m\u001b[K~~~~~~~~~~~~~~~~~~~~~~~~~~^~~~~~~~~~~~~~~~~~~~~~\u001b[m\u001b[K\n","\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/convert-llama2c-to-ggml/convert-llama2c-to-ggml.cpp ggml.o llama.o k_quants.o ggml-cuda.o ggml-alloc.o -o convert-llama2c-to-ggml  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/simple/simple.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o simple  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/save-load-state/save-load-state.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o save-load-state  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  -Iexamples/server examples/server/server.cpp ggml.o llama.o common.o grammar-parser.o k_quants.o ggml-cuda.o ggml-alloc.o -o server  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib  \n","g++ --shared -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/embd-input/embd-input-lib.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o libembdinput.so  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/embd-input/embd-input-test.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o embd-input-test  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib  -L. -lembdinput\n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/gguf/gguf.cpp ggml.o llama.o k_quants.o ggml-cuda.o ggml-alloc.o -o gguf  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/llama-bench/llama-bench.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o llama-bench  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/baby-llama/baby-llama.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o baby-llama  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/beam-search/beam-search.cpp ggml.o llama.o common.o k_quants.o ggml-cuda.o ggml-alloc.o -o beam-search  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","g++ -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c++11 -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wno-unused-function -Wno-multichar -Wno-format-truncation -Wno-array-bounds -pthread -march=native -mtune=native  examples/speculative/speculative.cpp ggml.o llama.o common.o grammar-parser.o k_quants.o ggml-cuda.o ggml-alloc.o -o speculative  -lcublas -lculibos -lcudart -lcublasLt -lpthread -ldl -lrt -L/usr/local/cuda/lib64 -L/opt/cuda/lib64 -L/targets/x86_64-linux/lib \n","cc -I. -Icommon -DNDEBUG -DGGML_USE_K_QUANTS -DGGML_USE_CUBLAS -I/usr/local/cuda/include -I/opt/cuda/include -I/targets/x86_64-linux/include  -O3 -std=c11   -fPIC -Wall -Wextra -Wpedantic -Wcast-qual -Wdouble-promotion -Wshadow -Wstrict-prototypes -Wpointer-arith -Wmissing-prototypes -Werror=implicit-int -Wno-unused-function -pthread -march=native -mtune=native  -c tests/test-c.c -o tests/test-c.o\n","/home/user\n","* Using config: Namespace(input=PosixPath('/home/user/.cache/huggingface/hub/models--TheBloke--Llama-2-70B-chat-GGML/snapshots/2eb277ec3007dcd8e5978ab07dbb235c90663ed1/llama-2-70b-chat.ggmlv3.q4_0.bin'), output=PosixPath('/home/user/.cache/huggingface/hub/models--TheBloke--Llama-2-70B-chat-GGML/snapshots/2eb277ec3007dcd8e5978ab07dbb235c90663ed1/llama-2-70b-chat.ggufv3.q4_1.bin'), name=None, desc=None, gqa=8, eps='1e-5', context_length=4096, model_metadata_dir=None, vocab_dir=None, vocabtype='spm')\n","\n","=== WARNING === Be aware that this conversion script is best-effort. Use a native GGUF model if possible. === WARNING ===\n","\n","* Scanning GGML input file\n","* File format: GGJTv3 with ftype MOSTLY_Q4_0\n","* GGML model hyperparameters: <Hyperparameters: n_vocab=32000, n_embd=8192, n_mult=4096, n_head=64, n_layer=80, n_rot=128, n_ff=28672, ftype=MOSTLY_Q4_0>\n","\n","=== WARNING === Special tokens may not be converted correctly. Use --model-metadata-dir if possible === WARNING ===\n","\n","- Guessed n_kv_head = 8 based on GQA 8\n","* Preparing to save GGUF file\n","* Adding model parameters and KV items\n","* Adding 32000 vocab item(s)\n","* Adding 723 tensor(s)\n","    gguf: write header\n","    gguf: write metadata\n","    gguf: write tensors\n","* Successful completion. Output saved to: /home/user/.cache/huggingface/hub/models--TheBloke--Llama-2-70B-chat-GGML/snapshots/2eb277ec3007dcd8e5978ab07dbb235c90663ed1/llama-2-70b-chat.ggufv3.q4_1.bin\n"]}],"source":["# Compile llama.cpp\n","!git clone https://github.com/ggerganov/llama.cpp\n","%cd llama.cpp\n","!make LLAMA_CUBLAS=1\n","%cd ../\n","\n","# Convert the model from GGML to GGUF\n","!python3 llama.cpp/convert-llama-ggml-to-gguf.py -i {llama_model_path_GGML} -o {llama_model_path_GGUF} --eps 1e-5 --context-length 4096 --gqa 8"]},{"cell_type":"markdown","id":"13f48edf","metadata":{"id":"13f48edf"},"source":["## 3.3 Dataset"]},{"cell_type":"code","execution_count":null,"id":"ee80e704","metadata":{"id":"ee80e704"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"h2oai/h2ogpt-gm-oasst1-en-2048-falcon-40b-v2\")"]},{"cell_type":"code","execution_count":null,"id":"ccadae5f","metadata":{"id":"ccadae5f"},"outputs":[],"source":["def preprocess_dataset(example):\n","    instruction = f\"{example['instruction']}\\n\\n{example['context']}\" if example['context'] else f\"{example['instruction']}\"\n","    return {'instruction': tokenizer.decode(tokenizer.encode(instruction, max_length=1024, truncation=True))}\n","\n","data = load_dataset(\n","    datasets[0],\n","    split=\"train\",\n",").map(\n","    preprocess_dataset,\n","    remove_columns=['context', 'response']\n",")"]},{"cell_type":"markdown","id":"9b543972","metadata":{"id":"9b543972"},"source":["## 3.4 nltk Stopwords"]},{"cell_type":"code","execution_count":null,"id":"9f1ede10","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9f1ede10","outputId":"925570a4-89f2-44de-9005-418b81cf4264"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/user/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /home/user/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"data":{"text/plain":["True"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"markdown","id":"054b0e56","metadata":{"id":"054b0e56"},"source":["## 3.5 Helpers"]},{"cell_type":"markdown","id":"704079af","metadata":{"id":"704079af"},"source":["### 3.5.1 Generate New Tokens"]},{"cell_type":"code","execution_count":null,"id":"2f19ae04","metadata":{"id":"2f19ae04"},"outputs":[],"source":["def falcon_generate(prompt, temp=0.8, timeout=600):\n","    global falcon_model_path\n","\n","    prompt = prompt.replace(\"\\\"\", \"\\\\\\\"\")\n","    cmd = f\"ggllm.cpp/falcon_main -t 22 -ngl 60 -b 512 --temp {temp} -m {falcon_model_path} -p \\\"{prompt}\\\"\"\n","\n","    def run_cmd(cmd, result):\n","        try:\n","            output = subprocess.check_output(cmd, shell=True)\n","            result['output'] = output\n","        except subprocess.CalledProcessError as e:\n","            result['error'] = str(e)\n","\n","    # Create a multiprocessing Manager to share data between processes\n","    manager = multiprocessing.Manager()\n","    result = manager.dict()\n","\n","    # Create a separate process to run the command\n","    process = multiprocessing.Process(target=run_cmd, args=(cmd, result))\n","    process.start()\n","    process.join(timeout=timeout)  # Wait for the process to finish or timeout\n","\n","    if process.is_alive():\n","        # If the process is still running after the timeout, terminate it\n","        process.terminate()\n","        process.join()\n","        return \"\"\n","\n","    if 'error' in result:\n","        return \"\"\n","\n","    out = result.get('output', b'') \\\n","        .decode(\"utf-8\").replace(\"<|endoftext|>\", \"\") \\\n","        .strip().strip('\\n') \\\n","        .split(\"<bot>:\", 1)[1].strip().strip('\\n').replace(\"\\\\\\\"\", \"\\\"\")\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"id":"9ca54cba","metadata":{"id":"9ca54cba"},"outputs":[],"source":["def llama_generate(prompt):\n","    global llama_model_path_GGUF\n","\n","    prompt = prompt.replace(\"\\\"\", \"\\\\\\\"\")\n","    prompt = f\"<s>[INST]{prompt}[/INST]\"\n","    cmd = f\"llama.cpp/main -c 2048 -t 22 -m {llama_model_path_GGUF} -ngl 83 -p \\\"{prompt}\\\"\"\n","\n","    try:\n","        output = subprocess.check_output(cmd, shell=True)\n","    except subprocess.CalledProcessError:\n","        return \"\"\n","\n","    out = output.decode(\"utf-8\").strip().strip('\\n').split(\"[/INST]\", 1)[1].replace(\"\\\\\\\"\", \"\\\"\")\n","\n","    return out"]},{"cell_type":"markdown","id":"f74c1047","metadata":{"id":"f74c1047","tags":[]},"source":["### 3.5.2 InstructionEvolution Class"]},{"cell_type":"code","execution_count":null,"id":"8a4a29ab","metadata":{"id":"8a4a29ab","tags":[]},"outputs":[],"source":["class InstructionEvolution:\n","    def __init__(self, initial_instructions, config=None):\n","        self.pool = initial_instructions\n","        self.evolved_dataset = {\n","            'instruction': [],\n","            'response': [],\n","            'category': [],\n","            'evolution_strategy': [],\n","            'in-depth-evolving_operation': [],\n","            'epoch': []\n","\n","        }\n","        self.config = {\n","            \"strategy\": None,\n","            \"in_depth_evolution_operation\": None,\n","            \"prompt\": None\n","        }\n","\n","    def select_evolution_strategy(self):\n","        logger.info(\"Selecting evolution strategy\")\n","        strategies = [\n","            (0, \"in-depth-evolving\"),\n","            (1, \"in-breadth-evolving\"),\n","        ]\n","\n","        self.config[\"strategy\"] = random.choice(strategies)\n","        logger.info(f\"Evolution strategy: {self.config['strategy'][1]}\")\n","\n","        return self\n","\n","    def select_in_depth_evolution_operation(self):\n","        logger.info(\"Selecting in-depth evolution operation\")\n","        operations = [\n","            (0, \"add-constraints\"),\n","            (1, \"deepening\"),\n","            (2, \"concretizing\"),\n","            (3, \"increase-reasoning-steps\")\n","        ]\n","\n","        self.config[\"in_depth_evolution_operation\"] = random.choice(operations)\n","        logger.info(f\"In-depth evolution operation: {self.config['in_depth_evolution_operation'][1]}\")\n","\n","        return self\n","\n","    def format_prompt_with_in_depth_evolution_operation(self):\n","        match self.config['in_depth_evolution_operation'][0]:\n","            case 0:\n","                self.config[\"prompt\"] = self.config[\"prompt\"].format(operation=\"by adding one more constraints/requirements into #Given Prompt#\")\n","            case 1:\n","                self.config[\"prompt\"] = self.config[\"prompt\"].format(operation=\"if #Given Prompt# contains inquiries about certain issues, the depth and breadth of the inquiry can be increased.\")\n","            case 2:\n","                self.config[\"prompt\"] = self.config[\"prompt\"].format(operation=\"by replacing general concepts with more specific concepts.\")\n","            case 3:\n","                self.config[\"prompt\"] = self.config[\"prompt\"].format(operation=\"if #Given Prompt# can be solved with just a few simple thinking processes, you can rewrite it to explicitly request multiple-step reasoning.\")\n","\n","        return self\n","\n","    def generate_prompt(self, instruction):\n","        match self.config[\"strategy\"][0]:\n","            case 0:\n","                self.config[\"prompt\"] = \"\"\"<human>: I want you to act as a prompt rewriter.\n","Your objective is to rewrite the #Given Prompt# into a more complex version.\n","But the rewritten prompt must be reasonable and must be understood and responded by humans.\n","Your rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please do not omit the context in #Given Prompt#.\n","You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.\n","‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in #Rewritten Prompt#\n","You SHOULD complicate the given prompt {operation}\n","#Given Prompt#:\n","{instruction}\n","<bot>: #Rewritten Prompt#:\"\"\"\n","\n","                self.config[\"prompt\"] = self.config[\"prompt\"].format(operation=\"{operation}\", instruction=instruction)\n","                self.format_prompt_with_in_depth_evolution_operation()\n","            case 1:\n","                self.config[\"prompt\"] = \"\"\"<human>: I want you to act as a prompt creator.\n","Your goal is to draw inspiration from the #Given Prompt# to create a brand new prompt.\n","This new prompt should belong to the same domain as the #Given Prompt# but be even more rare.\n","The LENGTH and difficulty level of the #Created Prompt# should be similar to that of the #Given Prompt#.\n","The #Created Prompt# must be reasonable and must be understood and responded by humans.\n","‘#Given Prompt#’, ‘#Created Prompt#’, ‘given prompt’ and ‘created prompt’ are not allowed to appear in #Created Prompt#.\n","Your response only contains the #Created Prompt# and no explanation of the new prompt. Do not provide a response to either the #Given Prompt# or the #Created Prompt#.\n","#Given Prompt#:\n","{instruction}\n","<bot>: #Created Prompt#:\"\"\"\n","\n","                self.config[\"prompt\"] = self.config[\"prompt\"].format(instruction=instruction)\n","\n","        print(f\"Prompt: {self.config['prompt']}\")\n","        return self\n","\n","    def example_generator(self, generate):\n","        logger.info(\"Generating example\")\n","        instruction = generate(self.config[\"prompt\"]).replace(\"#Rewritten Prompt#:\", \"\").replace(\"#Created Prompt#:\", \"\").strip().strip('\\n')\n","        print(instruction)\n","\n","        logger.info(\"Generating response\")\n","        response = generate(f\"<human>: {instruction}\\n<bot>: \")\n","        print(response)\n","\n","        return instruction, response\n","\n","    def instruction_evolver(self, instruction, generate):\n","        return self \\\n","            .generate_prompt(instruction) \\\n","            .example_generator(generate)\n","\n","\n","    def has_instruction_evolved(self, original_instruction, evolved_instruction, response, generate):\n","\n","        def has_information_gain(original_instruction, evolved_instruction, generate, counter=0):\n","            if counter > 5:\n","                return False\n","\n","            equality_check_prompt = f\"\"\"<human>: Do you think the following two instructions are equal to each other in that they meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: {original_instruction}\n","The Second Prompt: {evolved_instruction}\n","Your response should be either equal or not equal.\n","<bot>: The two prompts are \"\"\"\n","            print(equality_check_prompt)\n","            model_output = generate(equality_check_prompt, temp=0.0).lower().replace(\"*\", \"\")\n","            print(model_output)\n","\n","            if \"not equal\" in model_output:\n","                return True\n","            elif \"equal\" in nltk.word_tokenize(model_output):\n","                return False\n","            else:\n","                return has_information_gain(original_instruction, evolved_instruction, generate, counter+1)\n","\n","\n","        def is_response_difficult(response):\n","            return 'sorry' in response and len(nltk.word_tokenize(response)) < 80\n","\n","        def contains_only_punctuation_and_stop_words(response):\n","            stop_words = set(stopwords.words('english'))\n","            words = nltk.word_tokenize(response)\n","            return all(word in stop_words or word in string.punctuation for word in words)\n","\n","        def contains_disallowed_phrases(instruction):\n","            disallowed_phrases = [\n","                \"#Given Prompt#\", \"#Created Prompt#\", \"#Rewritten Prompt#\",\n","                \"given prompt\", \"created prompt\", \"rewritten prompt\"]\n","\n","            return any(phrase in instruction for phrase in disallowed_phrases)\n","\n","        if \\\n","        has_information_gain(original_instruction, evolved_instruction, generate) and \\\n","        not is_response_difficult(response) and \\\n","        not contains_only_punctuation_and_stop_words(response) and \\\n","        not contains_disallowed_phrases(evolved_instruction):\n","            return True\n","        else:\n","            return False\n","\n","    def save_dataset(self, epoch, category, file_name_manual_epoch=\"\", file_name_append_tag=\"\"):\n","        filename = os.path.join(\n","            \"evolved\",\n","            category,\n","            f\"\"\"{epoch if not file_name_manual_epoch else file_name_manual_epoch}_{self.config['strategy'][1]}{f\"_{self.config['in_depth_evolution_operation'][1]}\" if self.config['in_depth_evolution_operation'] else ''}{f\"_{file_name_append_tag}\" if file_name_append_tag else \"\"}.json\"\"\")\n","\n","        if not os.path.exists(os.path.dirname(filename)):\n","            os.makedirs(os.path.dirname(filename))\n","\n","        with open(filename, \"w\") as f:\n","            json.dump(self.evolved_dataset, f)\n","\n","\n","    def check_and_save_dataset(self, epoch, category, file_name_manual_epoch=\"\", file_name_append_tag=\"\"):\n","        global time0\n","\n","        if len(self.evolved_dataset['instruction']) % 5 >= 0 or time() - time0 >= 300:\n","            logger.info(\"Saving...\")\n","\n","            self.save_dataset(epoch, category, file_name_manual_epoch, file_name_append_tag)\n","            time0 = time()\n","            return True\n","        else:\n","            print(time() - time0)\n","            return False\n","\n","    def clear_evolved_instructions(self):\n","        self.evolved_dataset = {\n","            'instruction': [],\n","            'response': [],\n","            'category': [],\n","            'evolution_strategy': [],\n","            'in-depth-evolving_operation': [],\n","            'epoch': []\n","\n","        }\n","\n","\n","    def evolve(self, example_generate, eval_generate, category, file_name_manual_epoch=\"\", file_name_append_tag=\"\"):\n","        for epoch in tqdm(range(NUM_EPOCHS), desc=\"Evolving\", unit=\"epoch\"):\n","            new_pool = []\n","\n","            self.select_evolution_strategy()\n","            if self.config[\"strategy\"][0] == 0:\n","                self.select_in_depth_evolution_operation()\n","\n","            for instruction in tqdm(self.pool, desc=\"Instruction\", unit=\"instruction\"):\n","                try:\n","                    evolved_instruction, response = self.instruction_evolver(instruction, example_generate)\n","                    if self.has_instruction_evolved(instruction, evolved_instruction, response, eval_generate):\n","                        logger.info(\"Instruction Evolved\")\n","                        print(f\"Instruction Evolved: {evolved_instruction}\\n\\nResponse: {response}\")\n","\n","                        self.evolved_dataset['instruction'].append(evolved_instruction)\n","                        self.evolved_dataset['response'].append(response)\n","                        self.evolved_dataset['category'].append(category)\n","                        self.evolved_dataset['evolution_strategy'].append(self.config[\"strategy\"][1])\n","                        if self.config[\"in_depth_evolution_operation\"]:\n","                            self.evolved_dataset['in-depth-evolving_operation'].append(self.config[\"in_depth_evolution_operation\"][1])\n","                        else:\n","                            self.evolved_dataset['in-depth-evolving_operation'].append(\"\")\n","                        self.evolved_dataset['epoch'].append(epoch)\n","\n","                        new_pool.append(evolved_instruction)\n","\n","                        saved = self.check_and_save_dataset(epoch, category, file_name_manual_epoch, file_name_append_tag)\n","                        if saved:\n","                            logger.info(\"Saved\")\n","                    else:\n","                        logger.info(\"Instruction Not Evolved\")\n","                        print(f\"Instruction Not Evolved: {evolved_instruction}\")\n","\n","                        new_pool.append(instruction)\n","\n","                        with open(\"unevolved_instructions.txt\", \"a\") as f:\n","                            f.write(\"------------------------------------------------------------------------------\\n\")\n","                            f.write(f\"{epoch}, {category}\\n\")\n","                            f.write(\"Instruction Not Evolved\\n\")\n","                            f.write(\"------------------------------------------------------------------------------\\n\")\n","\n","                            f.write(f\"{instruction}\\n\")\n","                            f.write(\"========================================\\n\")\n","                            f.write(f\"{evolved_instruction}\\n\")\n","                            f.write(\"========================================\\n\")\n","                            f.write(f\"{response}\\n\")\n","                            f.write(\"\\n\\n\\n\")\n","\n","                    clear_output(wait=True)\n","                except:\n","                    pass\n","\n","            self.save_dataset(epoch, category, file_name_manual_epoch, file_name_append_tag)\n","            self.pool = new_pool\n","            self.clear_evolved_instructions()"]},{"cell_type":"markdown","id":"e444a162","metadata":{"tags":[],"id":"e444a162"},"source":["# Evolutions"]},{"cell_type":"code","execution_count":null,"id":"76ab3c12","metadata":{"id":"76ab3c12"},"outputs":[],"source":["def evolve_category():\n","    global category\n","    global NUM_EPOCHS\n","    global start\n","    global end\n","    global file_name_manual_epoch\n","    global starting_data\n","    global data\n","\n","    file_name_append_tag = f\"{start}-{end}\"\n","\n","    if starting_data:\n","        evolve_data = starting_data\n","    else:\n","        category_data = data.filter(lambda x: x['category'] == category)\n","        evolve_data = category_data['instruction'][start:end]\n","\n","    category_evolver = InstructionEvolution(evolve_data)\n","\n","    print(len(category_evolver.pool), category_evolver.pool[:10])\n","\n","    time0 = time()\n","    category_evolver.evolve(\n","        falcon_generate,\n","        falcon_generate,\n","        category,\n","        file_name_manual_epoch=file_name_manual_epoch,\n","        file_name_append_tag=file_name_append_tag)"]},{"cell_type":"markdown","id":"f0d76d94","metadata":{"tags":[],"id":"f0d76d94"},"source":["## brainstorming"]},{"cell_type":"code","execution_count":null,"id":"3c9fd0a9","metadata":{"id":"3c9fd0a9","outputId":"cdd96981-6708-4b56-ea01-4591cdd0d6a9","colab":{"referenced_widgets":["0fffcabf4f474f96a598fc262ea1b259"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0fffcabf4f474f96a598fc262ea1b259","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["1766"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["category = \"brainstorming\"\n","\n","len(data.filter(lambda x: x['category'] == category))"]},{"cell_type":"code","execution_count":null,"id":"276e0dc3","metadata":{"id":"276e0dc3"},"outputs":[],"source":["NUM_EPOCHS = 1\n","start = 0\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"29a321c9","metadata":{"id":"29a321c9","outputId":"0b274188-87e9-4e2a-bb75-d52c4dedb734"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 100/100 [1:53:13<00:00, 67.93s/instruction]\u001b[A\n","Evolving: 100%|██████████| 2/2 [4:13:28<00:00, 7604.22s/epoch]  \n"]}],"source":["category = \"brainstorming\"\n","\n","NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"1ceb34f4","metadata":{"tags":[],"id":"1ceb34f4"},"source":["## open_qa"]},{"cell_type":"code","execution_count":null,"id":"83086125","metadata":{"id":"83086125","outputId":"718b050d-c928-4e4a-c035-04b8c864e1ae","colab":{"referenced_widgets":["6187e1b79ce74472857ed6a20aafe28e"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6187e1b79ce74472857ed6a20aafe28e","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["3742"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["category = \"open_qa\"\n","\n","len(data.filter(lambda x: x['category'] == category))"]},{"cell_type":"code","execution_count":null,"id":"d490e421","metadata":{"id":"d490e421"},"outputs":[],"source":["NUM_EPOCHS = 1\n","start = 0\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"73881409","metadata":{"id":"73881409","outputId":"9244cb48-84bc-4968-f16a-1fe6bd50bab3"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 100/100 [1:38:51<00:00, 59.31s/instruction]\u001b[A\n","Evolving: 100%|██████████| 2/2 [3:15:02<00:00, 5851.25s/epoch]  \n"]}],"source":["NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"56ec8437","metadata":{"id":"56ec8437","outputId":"b6a042dc-c009-4234-c282-4cfe5198668c"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 100/100 [2:07:47<00:00, 76.68s/instruction]\u001b[A\n","Evolving: 100%|██████████| 3/3 [6:35:08<00:00, 7902.78s/epoch]  \n"]}],"source":["NUM_EPOCHS = 3\n","start = 700\n","end = 800\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"fe8d6022","metadata":{"tags":[],"id":"fe8d6022"},"source":["## closed_qa"]},{"cell_type":"code","execution_count":null,"id":"9410ebdc","metadata":{"id":"9410ebdc","outputId":"19c0c1a5-796d-450d-a45b-afa70430ec94","colab":{"referenced_widgets":["ac9a2d8e350b45ebaf13dafd05036c71"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ac9a2d8e350b45ebaf13dafd05036c71","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"}],"source":["category = \"close_qa\"\n","\n","NUM_EPOCHS = 1\n","start = 0\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"99c0ec8a","metadata":{"id":"99c0ec8a"},"outputs":[],"source":["category = \"closed_qa\"\n","\n","NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"dc1cf4df","metadata":{"tags":[],"id":"dc1cf4df"},"source":["## classification"]},{"cell_type":"code","execution_count":null,"id":"ea63f997","metadata":{"id":"ea63f997","outputId":"30018af6-635b-4ae8-fbcc-b863b17771b7","colab":{"referenced_widgets":["94be6d6869334039b994d30f6f6e28fd"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"94be6d6869334039b994d30f6f6e28fd","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["2136"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["category = \"classification\"\n","\n","len(data.filter(lambda x: x['category'] == category))"]},{"cell_type":"code","execution_count":null,"id":"e5e0f38d","metadata":{"id":"e5e0f38d","outputId":"7f01fb95-a707-4386-8de2-8379cb66a358"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 82/82 [1:09:21<00:00, 50.75s/instruction]\u001b[A\n","Evolving: 100%|██████████| 1/1 [1:09:21<00:00, 4161.59s/epoch]\n"]}],"source":["category = \"classification\"\n","\n","NUM_EPOCHS = 1\n","start = 18\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"cf2259c0","metadata":{"id":"cf2259c0","outputId":"e4e9282e-581b-4287-a98d-6d4ca1d66d3c"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 100/100 [1:59:33<00:00, 71.74s/instruction]\u001b[A\n","Evolving: 100%|██████████| 2/2 [3:48:49<00:00, 6864.95s/epoch]  \n"]}],"source":["NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"ee66c4bc","metadata":{"tags":[],"id":"ee66c4bc"},"source":["## information_extraction"]},{"cell_type":"code","execution_count":null,"id":"95ed0b4a","metadata":{"id":"95ed0b4a","outputId":"6f903170-2ce6-4ce9-9f65-4baa85df5add"},"outputs":[{"data":{"text/plain":["1506"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["category = \"information_extraction\"\n","\n","len(data.filter(lambda x: x['category'] == category))"]},{"cell_type":"code","execution_count":null,"id":"a37406b9","metadata":{"id":"a37406b9","outputId":"8b593fee-4d39-42e0-fdcc-46f68a970322"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 12/12 [11:39<00:00, 58.31s/instruction]\u001b[A\n","Evolving: 100%|██████████| 1/1 [11:39<00:00, 699.70s/epoch]\n"]}],"source":["NUM_EPOCHS = 1\n","start = 88\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"71b814d9","metadata":{"id":"71b814d9"},"outputs":[],"source":["NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"94163fc1","metadata":{"id":"94163fc1"},"outputs":[],"source":["NUM_EPOCHS = 2\n","start = 700\n","end = 800\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"9f0d8401","metadata":{"id":"9f0d8401"},"source":["## summarization"]},{"cell_type":"code","execution_count":null,"id":"63d599af","metadata":{"id":"63d599af","outputId":"b3575e04-ca63-4415-82cc-e0ab04b7ea55"},"outputs":[{"data":{"text/plain":["1188"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["category = \"summarization\"\n","\n","len(data.filter(lambda x: x['category'] == category))"]},{"cell_type":"code","execution_count":null,"id":"4d3787c4","metadata":{"id":"4d3787c4"},"outputs":[],"source":["NUM_EPOCHS = 1\n","start = 0 + 18\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"48234e7d","metadata":{"id":"48234e7d"},"outputs":[],"source":["NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"3882f918","metadata":{"id":"3882f918"},"outputs":[],"source":["NUM_EPOCHS = 2\n","start = 800\n","end = 900\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"7f7ac563","metadata":{"id":"7f7ac563","outputId":"c03f2551-efe6-4a23-f4b5-d71ace81cdad"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction:  71%|███████   | 71/100 [2:11:30<1:37:40, 202.10s/instruction]\u001b[A"]},{"name":"stdout","output_type":"stream","text":["Prompt: <human>: I want you to act as a prompt rewriter.\n","Your objective is to rewrite the #Given Prompt# into a more complex version.\n","But the rewritten prompt must be reasonable and must be understood and responded by humans.\n","Your rewriting cannot omit the non-text parts such as the table and code in #Given Prompt#:. Also, please do not omit the context in #Given Prompt#.\n","You should try your best not to make the #Rewritten Prompt# become verbose, #Rewritten Prompt# can only add 10 to 20 words into #Given Prompt#.\n","‘#Given Prompt#’, ‘#Rewritten Prompt#’, ‘given prompt’ and ‘rewritten prompt’ are not allowed to appear in #Rewritten Prompt#\n","You SHOULD complicate the given prompt by replacing general concepts with more specific concepts.\n","#Given Prompt#:\n","Please give me a short bulleted list of key points about Mars.\n","<bot>: #Rewritten Prompt#:\n"]},{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 39885.00 MB  of 40338.00 MB (in use:  452.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: mem required  =  563.19 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 60 of 60 layers to GPU, weights offloaded 33426.75 MB\n","falcon_model_load_internal: estimated VRAM usage: 35181 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |   7403 MB |  32934 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.80 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |   214 |    1694579356 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n","\n","falcon_print_timings:        load time =  6712.67 ms\n","falcon_print_timings:      sample time =    99.32 ms /    55 runs   (    1.81 ms per token,   553.77 tokens per second)\n","falcon_print_timings: batch eval time =  2639.01 ms /   214 tokens (   12.33 ms per token,    81.09 tokens per second)\n","falcon_print_timings:        eval time =  5515.49 ms /    54 runs   (  102.14 ms per token,     9.79 tokens per second)\n","falcon_print_timings:       total time =  8284.58 ms\n"]},{"name":"stdout","output_type":"stream","text":["Please give me a detailed list of notable facts about Mars, including its physical properties, geological features, and any significant events in its history. Also, please include any relevant cultural or social aspects of the planet that have been influenced by human exploration or colonization efforts.\n"]},{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 39877.00 MB  of 40338.00 MB (in use:  460.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: mem required  =  563.19 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 60 of 60 layers to GPU, weights offloaded 33426.75 MB\n","falcon_model_load_internal: estimated VRAM usage: 35181 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |   7361 MB |  32976 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.80 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |    61 |    1694579372 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","<human>: Do you think the following two instructions are equal to each other in that they meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: Please give me a short bulleted list of key points about Mars.\n","The Second Prompt: Please give me a detailed list of notable facts about Mars, including its physical properties, geological features, and any significant events in its history. Also, please include any relevant cultural or social aspects of the planet that have been influenced by human exploration or colonization efforts.\n","Your response should be either equal or not equal.\n","<bot>: The two prompts are \n"]},{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 6567.00 MB  of 40338.00 MB (in use: 33770.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: INFO: not enough VRAM to offload layer 7 (missing 28742 MB)\n","falcon_model_load_internal: INFO: 6 layers will be offloaded to GPU (layers 1 to 7)\n","falcon_model_load_internal: mem required  = 29722.05 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 6 of 60 layers to GPU, weights offloaded 4267.89 MB\n","falcon_model_load_internal: estimated VRAM usage: 6022 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |  35681 MB |   4656 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.00 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |   143 |    1694579972 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","<human>: Do you think the following two instructions are equal to each other in that they meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: Please give me a short bulleted list of key points about Mars.\n","The Second Prompt: Please give me a detailed list of notable facts about Mars, including its physical properties, geological features, and any significant events in its history. Also, please include any relevant cultural or social aspects of the planet that have been influenced by human exploration or colonization efforts.\n","Your response should be either equal or not equal.\n","<bot>: The two prompts are \n"]},{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 34861.00 MB  of 40338.00 MB (in use: 5476.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: INFO: not enough VRAM to offload layer 59 (missing 448 MB)\n","falcon_model_load_internal: INFO: 58 layers will be offloaded to GPU (layers 1 to 59)\n","falcon_model_load_internal: mem required  = 1113.36 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 58 of 60 layers to GPU, weights offloaded 32876.58 MB\n","falcon_model_load_internal: estimated VRAM usage: 34631 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |   7825 MB |  32512 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.00 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |   143 |    1694580573 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","<human>: Do you think the following two instructions are equal to each other in that they meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: Please give me a short bulleted list of key points about Mars.\n","The Second Prompt: Please give me a detailed list of notable facts about Mars, including its physical properties, geological features, and any significant events in its history. Also, please include any relevant cultural or social aspects of the planet that have been influenced by human exploration or colonization efforts.\n","Your response should be either equal or not equal.\n","<bot>: The two prompts are \n"]},{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 4879.00 MB  of 40338.00 MB (in use: 35458.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: INFO: not enough VRAM to offload layer 4 (missing 30430 MB)\n","falcon_model_load_internal: INFO: 3 layers will be offloaded to GPU (layers 1 to 4)\n","falcon_model_load_internal: mem required  = 31372.55 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 3 of 60 layers to GPU, weights offloaded 2617.39 MB\n","falcon_model_load_internal: estimated VRAM usage: 4372 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |   2307 MB |  38030 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.00 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |   143 |    1694581173 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","<human>: Do you think the following two instructions are equal to each other in that they meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: Please give me a short bulleted list of key points about Mars.\n","The Second Prompt: Please give me a detailed list of notable facts about Mars, including its physical properties, geological features, and any significant events in its history. Also, please include any relevant cultural or social aspects of the planet that have been influenced by human exploration or colonization efforts.\n","Your response should be either equal or not equal.\n","<bot>: The two prompts are \n"]},{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 36487.00 MB  of 40338.00 MB (in use: 3850.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: mem required  =  563.19 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 60 of 60 layers to GPU, weights offloaded 33426.75 MB\n","falcon_model_load_internal: estimated VRAM usage: 35181 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |   7345 MB |  32992 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.00 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |   143 |    1694581773 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n"]},{"name":"stdout","output_type":"stream","text":["\n","<human>: Do you think the following two instructions are equal to each other in that they meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: Please give me a short bulleted list of key points about Mars.\n","The Second Prompt: Please give me a detailed list of notable facts about Mars, including its physical properties, geological features, and any significant events in its history. Also, please include any relevant cultural or social aspects of the planet that have been influenced by human exploration or colonization efforts.\n","Your response should be either equal or not equal.\n","<bot>: The two prompts are \n"]},{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 6563.00 MB  of 40338.00 MB (in use: 33774.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: INFO: not enough VRAM to offload layer 7 (missing 28746 MB)\n","falcon_model_load_internal: INFO: 6 layers will be offloaded to GPU (layers 1 to 7)\n","falcon_model_load_internal: mem required  = 29722.05 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 6 of 60 layers to GPU, weights offloaded 4267.89 MB\n","falcon_model_load_internal: estimated VRAM usage: 6022 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |  35675 MB |   4662 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.00 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |   143 |    1694582373 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n"]}],"source":["NUM_EPOCHS = 3\n","start = 700\n","end = 800\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"89a489e3","metadata":{"id":"89a489e3"},"source":["## creative_writing"]},{"cell_type":"code","execution_count":null,"id":"19d733ee","metadata":{"id":"19d733ee","outputId":"ce899b48-37ab-48ff-b122-09310e61ac53"},"outputs":[{"data":{"text/plain":["709"]},"execution_count":10,"metadata":{},"output_type":"execute_result"}],"source":["category = \"creative_writing\"\n","\n","len(data.filter(lambda x: x['category'] == category))"]},{"cell_type":"code","execution_count":null,"id":"c15ae1c9","metadata":{"id":"c15ae1c9","outputId":"36bf6293-3f08-456e-9a31-4b09ee1cc093"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 82/82 [1:43:14<00:00, 75.55s/instruction]\u001b[A\n","Evolving: 100%|██████████| 1/1 [1:43:14<00:00, 6194.87s/epoch]\n"]}],"source":["NUM_EPOCHS = 1\n","start = 0 + 18\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"b9e29e83","metadata":{"id":"b9e29e83"},"outputs":[],"source":["NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"d218b408","metadata":{"id":"d218b408","outputId":"05b805db-c214-43c8-b77d-1a5695ebdf88"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 100/100 [3:46:25<00:00, 135.85s/instruction][A\n","Evolving: 100%|██████████| 3/3 [9:23:36<00:00, 11272.33s/epoch]  \n"]}],"source":["NUM_EPOCHS = 3\n","start = 500\n","end = 600\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"0f39856e","metadata":{"id":"0f39856e"},"source":["## general_qa"]},{"cell_type":"code","execution_count":null,"id":"d788ab37","metadata":{"id":"d788ab37","outputId":"9c91a90f-5aa6-4ecf-9e0c-d676fd397929","colab":{"referenced_widgets":["e8202334c39f4f4c94062413723162d3"]}},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e8202334c39f4f4c94062413723162d3","version_major":2,"version_minor":0},"text/plain":["Filter:   0%|          | 0/15011 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"text/plain":["2191"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["category = \"general_qa\"\n","\n","len(data.filter(lambda x: x['category'] == category))"]},{"cell_type":"code","execution_count":null,"id":"e77e99a2","metadata":{"id":"e77e99a2","outputId":"a89e7c4e-3dd1-46f9-8130-871f9e537783"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 100/100 [1:47:31<00:00, 64.51s/instruction]\u001b[A\n","Evolving: 100%|██████████| 1/1 [1:47:31<00:00, 6451.12s/epoch]\n"]}],"source":["NUM_EPOCHS = 1\n","start = 0\n","end = 100\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"449d8cd7","metadata":{"id":"449d8cd7"},"outputs":[],"source":["NUM_EPOCHS = 2\n","start = -101\n","end = -1\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"code","execution_count":null,"id":"db7a3972","metadata":{"id":"db7a3972","outputId":"e98f3b53-3244-49c7-98d1-a17d10bb2d81"},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","Instruction: 100%|██████████| 100/100 [2:15:24<00:00, 81.25s/instruction] \u001b[A\n","Evolving: 100%|██████████| 3/3 [6:06:39<00:00, 7333.22s/epoch]  \n"]}],"source":["NUM_EPOCHS = 3\n","start = 500\n","end = 600\n","file_name_manual_epoch = \"\"\n","starting_data = []\n","\n","evolve_category()"]},{"cell_type":"markdown","id":"1cee48f5","metadata":{"jp-MarkdownHeadingCollapsed":true,"tags":[],"id":"1cee48f5"},"source":["# Tests"]},{"cell_type":"code","execution_count":null,"id":"fb839753","metadata":{"id":"fb839753","outputId":"bce7cc26-7161-42ed-d629-603cf89bae6e"},"outputs":[{"name":"stdout","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 39853.00 MB  of 40338.00 MB (in use:  484.00 MB)\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: mem required  =    0.00 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 60 of 60 layers to GPU, weights offloaded 33426.75 MB\n","falcon_model_load_internal: estimated VRAM usage: 33459 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  151.00 MB, Scratch 1 =   40.25 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |   7327 MB |  33010 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.00 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |     1 |     0 |    93 |    1694281767 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n","Do you think the following two instructions are equal to each other, which meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: What is the world view on timber cutting?\n","The Second Prompt: What is the future outlook for the carpentry industry?\n","Respond with either \"equal\" or \"not equal\"\n","I think the two prompts are not equal to each other. The first prompt focuses on the environmental impact of timber cutting, while the second prompt focuses on the economic and technological trends in the carpentry industry. While both topics may be related to the broader issue of sustainability, they have different scopes and perspectives.<|endoftext|>\n","falcon_print_timings:        load time =  5267.87 ms\n","falcon_print_timings:      sample time =    98.89 ms /    57 runs   (    1.73 ms per token,   576.41 tokens per second)\n","falcon_print_timings:        eval time = 13564.83 ms /   149 runs   (   91.04 ms per token,    10.98 tokens per second)\n","falcon_print_timings:       total time = 13693.51 ms\n"]}],"source":["prompt = \"\"\"Do you think the following two instructions are equal to each other, which meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: What is the world view on timber cutting?\n","The Second Prompt: What is the future outlook for the carpentry industry?\n","Respond with either \"equal\" or \"not equal\"\n","I think the two prompts are\"\"\"\n","\n","prompt = prompt.replace(\"\\\"\", \"\\\\\\\"\")\n","\n","\n","!ggllm.cpp/falcon_main -t 22 -ngl 60 -b 1 --temp 0.0 -m {falcon_model_path} -p \"{prompt}\""]},{"cell_type":"code","execution_count":null,"id":"c8b9a0b4","metadata":{"id":"c8b9a0b4","outputId":"32a4ab52-d11d-4e25-ff72-720d225637e1"},"outputs":[{"name":"stderr","output_type":"stream","text":["main: build = 887 (e52ddf2)\n","falcon.cpp: loading model from /home/user/.cache/huggingface/hub/models--TheBloke--h2ogpt-gm-oasst1-en-2048-falcon-40b-v2-GGML/snapshots/67bac2c70c558b2a01231392c86fd88c56aea064/h2ogpt-falcon-40b.ggmlv3.q6_k.bin\n","falcon.cpp: file version 4\n","falcon.cpp: fallback for old file format. Loading BPE merges from tokenizer.json\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|          Info |     format | n_vocab |   n_bpe | n_ctx | n_embd |   n_head ; kv | n_layer | falcon | ftype |   n_ff |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","|               |    ggml v3 |   65024 |   64784 |  2048 |   8192 |     128 ;   8 |      60 | 40;40B |    18 |  32768 |\n","+---------------+------------+---------+---------+-------+--------+---------------+---------+--------+-------+--------+\n","falcon_model_load_internal: ggml ctx size =    0.00 MB (mmap size = 32734.00 MB)\n","falcon_model_load_internal: using CUDA for GPU acceleration\n","falcon_model_load_internal: VRAM free: 39843.00 MB  of 40338.00 MB (in use:  494.00 MB)\n","falcon_model_load_internal: INFO: using n_batch larger than 1 requires additional VRAM per device: 1754.00 MB\n","falcon_model_load_internal: allocating batch_size x 1 MB = 0 MB VRAM for the scratch buffer\n","falcon_model_load_internal: Offloading Output head tensor (416 MB)\n","falcon_model_load_internal: mem required  =  563.19 MB (+  720.00 MB per state)\n","falcon_model_load_internal: offloading 60 of 60 layers to GPU, weights offloaded 33426.75 MB\n","falcon_model_load_internal: estimated VRAM usage: 35181 MB\n","[==================================================] 100%  Tensors populated, CUDA ready \n","falcon_context_prepare: Context falcon_main RAM buffers - key_val =  240.00 MB, Compute =  256.00 MB, Scratch 0 =  831.00 MB, Scratch 1 =  168.00 MB \n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","| ID | Device                     1 found | VRAM Total | VRAM Free | VRAM Used | Split at  |    Device |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","|  0 | NVIDIA A100-SXM4-40GB              |   40338 MB |   7343 MB |  32994 MB |      0.0% |   Primary |\n","+----+------------------------------------+------------+-----------+-----------+-----------+-----------+\n","\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| Syst. Info | AVX | AVX2 | AVX512 | AVX512_VBMI | AVX512_VNNI | FMA | NEON | ARM_FMA | F16C | FP16_VA | SIMD | BLAS | SSE3 | VSX |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","| 22/22 thrd | 1   | 1    | 0      | 0           | 0           | 1   | 0    | 0       | 1    | 0       | 0    | 1    | 1    | 0   |\n","+------------+-----+------+--------+-------------+-------------+-----+------+---------+------+---------+------+------+------+-----+\n","\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|   Sampling | rpt_n | rpt_p | prs_p | frq_p | top_k | tfs_z | top_p | typ_p | temp | miro | mir_lr | mir_ent |\n","+------------+-------+-------+-------+-------+-------+-------+-------+-------+------+------+--------+---------+\n","|            |    64 | 1.100 | 0.000 | 0.000 |    40 | 1.000 | 0.950 | 1.000 | 0.10 |    0 | 0.1000 | 5.00000 |\n","+============+=======+=======+=======+=======+=======+=======+-------+-------+------+------+--------+---------+\n","| Generation |   Ctx | Batch |  Keep | Prom. |          Seed |             Finetune | Stop |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","|            |  2048 |   512 |     0 |   104 |    1694312764 |        OPENASSIST_V1 | #  1 |\n","+------------+-------+-------+-------+-------+---------------+----------------------+------+\n","\n","\n","\n","falcon_print_timings:        load time =  5503.09 ms\n","falcon_print_timings:      sample time =   114.45 ms /    68 runs   (    1.68 ms per token,   594.17 tokens per second)\n","falcon_print_timings: batch eval time =  1237.12 ms /   104 tokens (   11.90 ms per token,    84.07 tokens per second)\n","falcon_print_timings:        eval time =  6120.20 ms /    67 runs   (   91.35 ms per token,    10.95 tokens per second)\n","falcon_print_timings:       total time =  7507.78 ms\n"]},{"data":{"text/plain":["'The two prompts are **not equal**.\\n\\nThe first prompt is asking for a list of body weight exercises for core and abs, while the second prompt is asking for something that is not seen in the human body but is essential for life. These two prompts have different requirements and constraints, and they do not meet the same depth and breadth of inquiry.'"]},"execution_count":110,"metadata":{},"output_type":"execute_result"}],"source":["falcon_generate(\"\"\"<human>: Do you think the following two instructions are equal to each other in that they meet the following requirements:\n","1. They have same constraints and requirements.\n","2. They have same depth and breadth of the inquiry.\n","The First Prompt: What are some good body weight exercises for core and abs?\n","The Second Prompt: Name something that is not seen in the human body, but essential for life.\n","Your response should be either equal or not equal.\n","<bot>: The two prompts are \"\"\", temp=0.1)"]},{"cell_type":"code","execution_count":null,"id":"b5f9a71e","metadata":{"id":"b5f9a71e","tags":[]},"outputs":[],"source":["ie = InstructionEvolution(data.select(range(5, 8))['instruction'])"]},{"cell_type":"code","execution_count":null,"id":"f56ff7db","metadata":{"id":"f56ff7db","outputId":"1f739961-c48e-4a3c-d5ec-2c22e994bc0e"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3/3 [03:53<00:00, 77.99s/it] \n"]}],"source":["ie.evolve(falcon_generate, falcon_generate, \"test\")"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}